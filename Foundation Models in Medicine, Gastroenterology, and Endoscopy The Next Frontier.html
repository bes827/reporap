
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Foundation Models in Medicine, Gastroenterology, and Endoscopy: The Next Frontier?</title>
<style>

<style>
body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    background: linear-gradient(135deg, #f0f4ff, #fafafa);
    color: #2c2c2c;
    padding: 40px;
}

/* Mindmap container */
.mindmap {
    max-width: 900px;
    margin: auto;
    font-size: 16px;
    line-height: 1.7;
}

.mindmap ul {
    margin: 0.5em 0 1em 1em;
    padding-left: 1.5em;
    list-style: disc;
    color: #555;
    font-size: 0.95em;
    font-weight: 400;
    line-height: 1.6;
}

.mindmap li::marker {
    color: #a855f7; /* prettier bullets */
}

.mindmap li {
    margin-bottom: 6px;
    padding-left: 4px;
}


/* Collapsible sections */
.mindmap details {
    border-left: 4px solid #8b5cf6;
    background: #ffffff;
    margin: 15px 0;
    padding: 16px 20px;
    border-radius: 16px;
    box-shadow: 0 4px 12px rgba(139, 92, 246, 0.12);
    transition: all 0.3s ease;
    position: relative;
}

/* Hover glow */
.mindmap details:hover {
    box-shadow: 0 6px 18px rgba(139, 92, 246, 0.2);
    background: linear-gradient(135deg, #ffffff, #f3f0ff);
}

/* Summary styling */
.mindmap summary {
    cursor: pointer;
    font-size: 1.15em;
    font-weight: 600;
    list-style: none;
    position: relative;
    padding-left: 28px;
    color: #4c1d95;
}

/* Custom arrow */
.mindmap summary::before {
    content: "▶";
    position: absolute;
    left: 0;
    top: 2px;
    transition: transform 0.3s ease;
    font-size: 1em;
    color: #7c3aed;
}

.mindmap details[open] > summary::before {
    transform: rotate(90deg);
}

/* Titles */
.mindmap summary strong {
    display: block;
    font-size: 1.3em;
    color: #4c1d95;
}

.mindmap summary em {
    font-style: normal;
    color: #6b21a8;
    font-size: 0.95em;
}

/* Description bullets */
.mindmap ul {
    padding-left: 22px;
    margin-top: 10px;
    margin-bottom: 12px;
}

.mindmap li {
    margin-bottom: 6px;
    font-size: 0.97em;
    color: #444;
    list-style-type: disc;
    position: relative;
    padding-left: 4px;
}

/* Nesting indent style */
.mindmap details > div > details {
    margin-left: 20px;
    border-color: #a855f7;
    background: #f9f5ff;
}

/* Fade-in effect */
.mindmap details[open] > div {
    animation: fadeIn 0.3s ease-in;
}

/* Description box */
.mindmap .desc-box {
    background: #fdfcff;
    border: 1px solid #e5d9fb;
    border-left: 4px solid #c084fc;
    padding: 4px 6px;
    margin: 12px 0 18px 0;
    border-radius: 12px;
    box-shadow: 0 1px 3px rgba(160, 104, 255, 0.06);
    transition: background 0.3s ease;
}

.mindmap .desc-box:hover {
    background: #f9f4ff;
}


button {
    background-color: #7c3aed;
    color: white;
    border: none;
    padding: 10px 16px;
    margin: 0 6px;
    font-size: 1em;
    border-radius: 8px;
    cursor: pointer;
    transition: background 0.3s ease;
}
button:hover {
    background-color: #6b21a8;
}


@keyframes fadeIn {
    from { opacity: 0; transform: translateY(-3px); }
    to { opacity: 1; transform: translateY(0); }
}


#level-controls button {
    background-color: rgba(124, 58, 237, 0.7); /* semi-transparent purple */
    color: white;
    border: none;
    padding: 8px 14px;
    margin: 0 6px;
    font-size: 1em;
    border-radius: 8px;
    cursor: pointer;
    transition: background 0.3s ease, transform 0.2s;
}

#level-controls button:hover {
    background-color: rgba(107, 33, 168, 0.85); /* slightly darker on hover */
    transform: scale(1.05);
}

#level-controls {
    position: fixed;
    bottom: 20px;
    left: 50%;
    transform: translateX(-50%);
    background: rgba(124, 58, 237, 0.1);
    padding: 8px 14px;
    border-radius: 12px;
    box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
    z-index: 9999;
    backdrop-filter: blur(6px);
    cursor: move; /* shows move cursor */
}

</style>




</style>
</head>
<body>
<div id="level-controls" onmousedown="dragElement(this)">
    <button onclick="setMindmapLevel(1)">Level 1</button>
    <button onclick="setMindmapLevel(2)">Level 2</button>
    <button onclick="setMindmapLevel(3)">Level 3</button>
</div>

<div class="mindmap">

    <div class="mindmap">
        
        <details>
            <summary><strong>Foundation Models in Medicine, Gastroenterology, and Endoscopy: The Next Frontier?</strong></summary>
            <div>
                <ul><li>Journal: Gastroenterology</li><li>Publication Date: Accepted 2 October 2025</li><li>DOI: https://doi.org/10.1053/j.gastro.2025.10.001</li></ul>
                
        <details>
            <summary><strong>Knowledge Test</strong></summary>
            <div>
                <ul><li>1. What is the primary advantage of foundation models over traditional CNNs in GI endoscopy, especially when labeled datasets are limited?</li><li>2. GastroNet-5M was trained on how many unlabeled endoscopic images and from how many hospitals?</li><li>3. In performance benchmarks, for what percentage of classification tasks did GastroNet-5M rank first?</li><li>4. When fine-tuning data was reduced to 25% or less, what was the observed performance trend for GastroNet-5M compared to baseline models?</li><li>5. What is a key limitation of GastroNet-5M regarding its generalizability to diverse populations?</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Foundation Models (FMs): Overview</strong></summary>
            <div>
                <ul><li>FMs are changing AI landscape for natural language processing (NLP) &amp; computer vision in medicine.</li><li>Unlike traditional task-specific CNNs, FMs are pretrained on very large, broad datasets.</li><li>FMs learn general-purpose representations → adapted via fine-tuning/prompting for specific tasks.</li></ul>
                
        <details>
            <summary><strong>Self-Supervised Learning Methods</strong></summary>
            <div>
                <ul><li>FMs use self-supervised learning to learn general representations.</li><li>Methods include: next-token prediction, masked prediction, contrastive learning.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>FM vs. Traditional CNNs</strong></summary>
            <div>
                <ul><li>Traditional CNNs: Task-specific, often initialized w/ ImageNet-pretrained weights.</li><li>FMs: Pretrained on broad datasets, learn general representations.</li><li>Advantage in GI endoscopy: Labeled datasets often limited/hard to obtain; variability across sites/devices demands robust generalization.</li></ul>
                
            </div>
        </details>
        
            </div>
        </details>
        
        <details>
            <summary><strong>GastroNet-5M: A Landmark Endoscopy FM</strong></summary>
            <div>
                <ul><li>GastroNet-5M marks the next evolution for AI in GI endoscopy.</li><li>It&#x27;s an endoscopy domain-specific FM using self-supervised learning.</li><li>Trained w/ ≈4.82 million unlabeled endoscopic images from &gt;500,000 procedures.</li></ul>
                
        <details>
            <summary><strong>Training Details</strong></summary>
            <div>
                <ul><li>Data sourced from 8 Dutch hospitals.</li><li>Used DINOv2: A self-supervised training method.</li><li>DINOv2 produces ViT backbones w/ strong, transferable representations.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Performance Benchmarks</strong></summary>
            <div>
                <ul><li>Ranked 1st on 8 of 9 classification tasks.</li><li>Consistently outperformed competitors across all segmentation tasks.</li><li>High-stakes problems: Barrett’s neoplasia, gastric invasion depth estimation.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Data Efficiency</strong></summary>
            <div>
                <ul><li>When fine-tuning data ↓ to ≤25%, GastroNet-5M often maintained/exceeded full-data performance of baseline models.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Robustness</strong></summary>
            <div>
                <ul><li>Showed resilience under domain shift.</li><li>Examples: Different endoscope manufacturers, varying image qualities.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Advancement over Prior Work</strong></summary>
            <div>
                <ul><li>Earlier FMs (e.g., HyperKvasir-based, EndoFM, EndoMamba, PolypSegTrack) used single-center/heterogeneous public datasets.</li><li>Prior evaluations focused on limited downstream tasks.</li><li>GastroNet-5M: Large, domain-specific, multicenter collection; broad spectrum of clinically relevant applications.</li></ul>
                
            </div>
        </details>
        
            </div>
        </details>
        
        <details>
            <summary><strong>Clinical Applications &amp; Benefits of FMs</strong></summary>
            <div>
                <ul><li>FMs offer broad pattern recognition capacity.</li><li>Reduce annotation burdens + improve robustness → attractive for real-time CADe systems.</li><li>Potential to reshape endoscopic AI development by enabling transferability &amp; improving performance in low-data settings.</li></ul>
                
        <details>
            <summary><strong>Key Endoscopy Tasks</strong></summary>
            <div>
                <ul><li>Computer-aided detection (CADe) of colorectal polyps, bleeding, lesions.</li><li>IBD inflammation scoring.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>IBD Inflammation Scoring (ArgesFM)</strong></summary>
            <div>
                <ul><li>ArgesFM: ViT-based foundation encoder pretrained self-supervised on tens of millions of endoscopy frames.</li><li>Paired w/ lightweight spatiotemporal transformer head.</li><li>Better captures mucosal texture, bleeding, erosions, vascular patterns.</li><li>Improves consistency &amp; generalization across devices.</li><li>Reduces dependence on scarce frame-level labels.</li><li>Provides attention maps for quality control.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Task-Specific Requirements</strong></summary>
            <div>
                <ul><li>Polyp detection: Prioritizes speed &amp; robustness.</li><li>Dysplasia characterization: Demands greater explainability &amp; prospective validation.</li><li>GastroNet-5M outperformed baselines in these areas, but requires additional validation studies.</li></ul>
                
            </div>
        </details>
        
            </div>
        </details>
        
        <details>
            <summary><strong>Limitations &amp; Challenges of FMs</strong></summary>
            <div>
                <ul><li>GastroNet-5M was developed using static images; many endoscopic phenomena are dynamic.</li><li>No study has demonstrated FMs improve diagnostic decision-making or clinical endpoints.</li><li>Incremental gains in AUC/Dice scores ≠ better patient outcomes.</li></ul>
                
        <details>
            <summary><strong>Generalizability</strong></summary>
            <div>
                <ul><li>GastroNet-5M images sourced only from Dutch hospitals.</li><li>Generalizability to international populations, rare pathologies, diverse healthcare systems needs exploration.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Interpretability (&#x27;Black Box&#x27; Nature)</strong></summary>
            <div>
                <ul><li>FMs &amp; CNNs often &#x27;black boxes&#x27; → decision-making difficult to interpret.</li><li>Task-specific CNNs: More deterministic, outputs easier to validate.</li><li>FMs: Pretrained on massive, heterogeneous datasets → less predictable in edge cases, can introduce variability if not adequately fine-tuned.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Computational Demands</strong></summary>
            <div>
                <ul><li>FMs require greater computational resources to create &amp; deploy.</li><li>Challenge for both CNN- &amp; FM-based models, but FMs&#x27; large-scale pretraining exacerbates it.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Dataset Bias</strong></summary>
            <div>
                <ul><li>Affects both CNN- &amp; FM-based models.</li><li>FMs, through large-scale self-supervised pretraining, may be better positioned to overcome data limitations.</li></ul>
                
            </div>
        </details>
        
            </div>
        </details>
        
        <details>
            <summary><strong>Future Directions &amp; Integration</strong></summary>
            <div>
                <ul><li>Central question: How to combine &amp; adapt different architectures to specific clinical tasks.</li><li>Prospective clinical validation essential to ensure reliability &amp; improving outcomes that matter for clinical care.</li></ul>
                
        <details>
            <summary><strong>Combining Architectures</strong></summary>
            <div>
                <ul><li>Foundation backbones: Provide robust generalization.</li><li>CNNs: Support precise lesion characterization (e.g., fine texture cues for dysplasia recognition).</li><li>Transformers: Capture temporal context across procedures (for dynamic phenomena).</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Critical Next Steps</strong></summary>
            <div>
                <ul><li>Systematic testing of pretraining sources, fine-tuning strategies, &amp; performance.</li><li>Aligning architectures &amp; training strategies w/ the right clinical applications.</li></ul>
                
            </div>
        </details>
        
            </div>
        </details>
        
            </div>
        </details>
        
    </div>
    
</div>


<script>
function setMindmapLevel(level) {
    const allDetails = document.querySelectorAll('.mindmap details');

    // Close all
    allDetails.forEach(detail => detail.removeAttribute('open'));

    // Open based on depth
    allDetails.forEach(detail => {
        let depth = 0;
        let parent = detail.parentElement;
        while (parent && parent.tagName !== 'BODY') {
            if (parent.tagName === 'DETAILS') depth++;
            parent = parent.parentElement;
        }
        if (depth < level) detail.setAttribute('open', '');
    });
}

// Automatically open to level 1 on page load
window.addEventListener('DOMContentLoaded', () => {
    setMindmapLevel(1);
});

function dragElement(elmnt) {
    let pos1 = 0, pos2 = 0, pos3 = 0, pos4 = 0;

    const onMouseDown = (e) => {
        e.preventDefault();
        pos3 = e.clientX;
        pos4 = e.clientY;
        document.onmouseup = closeDragElement;
        document.onmousemove = elementDrag;
    };

    const elementDrag = (e) => {
        e.preventDefault();
        pos1 = pos3 - e.clientX;
        pos2 = pos4 - e.clientY;
        pos3 = e.clientX;
        pos4 = e.clientY;
        elmnt.style.top = (elmnt.offsetTop - pos2) + "px";
        elmnt.style.left = (elmnt.offsetLeft - pos1) + "px";
        elmnt.style.transform = "none"; // cancel centering transform
        elmnt.style.bottom = "auto"; // cancel fixed bottom
    };

    const closeDragElement = () => {
        document.onmouseup = null;
        document.onmousemove = null;
    };

    onMouseDown(event);
}
</script>


</body>
</html>
