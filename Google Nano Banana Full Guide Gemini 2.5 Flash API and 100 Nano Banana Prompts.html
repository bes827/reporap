
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Google Nano Banana [Full Guide] Gemini 2.5 Flash API and 100+ Nano Banana Prompts</title>
<style>

<style>
body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    background: linear-gradient(135deg, #f0f4ff, #fafafa);
    color: #2c2c2c;
    padding: 40px;
}

/* Mindmap container */
.mindmap {
    max-width: 900px;
    margin: auto;
    font-size: 16px;
    line-height: 1.7;
}

.mindmap ul {
    margin: 0.5em 0 1em 1em;
    padding-left: 1.5em;
    list-style: disc;
    color: #555;
    font-size: 0.95em;
    font-weight: 400;
    line-height: 1.6;
}

.mindmap li::marker {
    color: #a855f7; /* prettier bullets */
}

.mindmap li {
    margin-bottom: 6px;
    padding-left: 4px;
}


/* Collapsible sections */
.mindmap details {
    border-left: 4px solid #8b5cf6;
    background: #ffffff;
    margin: 15px 0;
    padding: 16px 20px;
    border-radius: 16px;
    box-shadow: 0 4px 12px rgba(139, 92, 246, 0.12);
    transition: all 0.3s ease;
    position: relative;
}

/* Hover glow */
.mindmap details:hover {
    box-shadow: 0 6px 18px rgba(139, 92, 246, 0.2);
    background: linear-gradient(135deg, #ffffff, #f3f0ff);
}

/* Summary styling */
.mindmap summary {
    cursor: pointer;
    font-size: 1.15em;
    font-weight: 600;
    list-style: none;
    position: relative;
    padding-left: 28px;
    color: #4c1d95;
}

/* Custom arrow */
.mindmap summary::before {
    content: "‚ñ∂";
    position: absolute;
    left: 0;
    top: 2px;
    transition: transform 0.3s ease;
    font-size: 1em;
    color: #7c3aed;
}

.mindmap details[open] > summary::before {
    transform: rotate(90deg);
}

/* Titles */
.mindmap summary strong {
    display: block;
    font-size: 1.3em;
    color: #4c1d95;
}

.mindmap summary em {
    font-style: normal;
    color: #6b21a8;
    font-size: 0.95em;
}

/* Description bullets */
.mindmap ul {
    padding-left: 22px;
    margin-top: 10px;
    margin-bottom: 12px;
}

.mindmap li {
    margin-bottom: 6px;
    font-size: 0.97em;
    color: #444;
    list-style-type: disc;
    position: relative;
    padding-left: 4px;
}

/* Nesting indent style */
.mindmap details > div > details {
    margin-left: 20px;
    border-color: #a855f7;
    background: #f9f5ff;
}

/* Fade-in effect */
.mindmap details[open] > div {
    animation: fadeIn 0.3s ease-in;
}

/* Description box */
.mindmap .desc-box {
    background: #fdfcff;
    border: 1px solid #e5d9fb;
    border-left: 4px solid #c084fc;
    padding: 4px 6px;
    margin: 12px 0 18px 0;
    border-radius: 12px;
    box-shadow: 0 1px 3px rgba(160, 104, 255, 0.06);
    transition: background 0.3s ease;
}

.mindmap .desc-box:hover {
    background: #f9f4ff;
}


button {
    background-color: #7c3aed;
    color: white;
    border: none;
    padding: 10px 16px;
    margin: 0 6px;
    font-size: 1em;
    border-radius: 8px;
    cursor: pointer;
    transition: background 0.3s ease;
}
button:hover {
    background-color: #6b21a8;
}


@keyframes fadeIn {
    from { opacity: 0; transform: translateY(-3px); }
    to { opacity: 1; transform: translateY(0); }
}


#level-controls button {
    background-color: rgba(124, 58, 237, 0.7); /* semi-transparent purple */
    color: white;
    border: none;
    padding: 8px 14px;
    margin: 0 6px;
    font-size: 1em;
    border-radius: 8px;
    cursor: pointer;
    transition: background 0.3s ease, transform 0.2s;
}

#level-controls button:hover {
    background-color: rgba(107, 33, 168, 0.85); /* slightly darker on hover */
    transform: scale(1.05);
}

#level-controls {
    position: fixed;
    bottom: 20px;
    left: 50%;
    transform: translateX(-50%);
    background: rgba(124, 58, 237, 0.1);
    padding: 8px 14px;
    border-radius: 12px;
    box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
    z-index: 9999;
    backdrop-filter: blur(6px);
    cursor: move; /* shows move cursor */
}

</style>




</style>
</head>
<body>
<div id="level-controls" onmousedown="dragElement(this)">
    <button onclick="setMindmapLevel(1)">Level 1</button>
    <button onclick="setMindmapLevel(2)">Level 2</button>
    <button onclick="setMindmapLevel(3)">Level 3</button>
</div>

<div class="mindmap">

    <div class="mindmap">
        
        <details>
            <summary><strong>Google Nano Banana [Full Guide] Gemini 2.5 Flash API and 100+ Nano Banana Prompts</strong></summary>
            <div>
                <ul><li>Article by Simranjeet Singh.</li><li>Published on Generative AI.</li><li>Publication Date: Sep 24, 2025.</li><li>Read time: 13 min.</li></ul>
                
        <details>
            <summary><strong>Knowledge Test</strong></summary>
            <div>
                <ul><li>1. What is the approximate token cost for generating a single image using Nano-Banana?</li><li>2. Which Google AI model forms the underlying architecture of Nano-Banana?</li><li>3. What is the primary mechanism Nano-Banana uses for image generation, starting from a noisy image?</li><li>4. What is the maximum number of input images Nano-Banana can combine for fusion tasks?</li><li>5. What invisible digital identifier is embedded in all AI-generated or edited images by Nano-Banana to promote trust and safety?</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Introduction to Nano-Banana</strong></summary>
            <div>
                <ul><li>Google‚Äôs Gemini 2.5 Flash Image AI.</li><li>Next-gen image editing/generation model.</li><li>Transforms single photo ‚Üí multiple creative outputs.</li></ul>
                
        <details>
            <summary><strong>Key Capabilities</strong></summary>
            <div>
                <ul><li>Blend multiple images.</li><li>Perform style transfers w/ precision.</li><li>Character-consistent transformations.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Trending Factors</strong></summary>
            <div>
                <ul><li>Viral image edits on social media.</li><li>Multi-image fusion ‚Üí surreal, high-quality outputs.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>API Integration</strong></summary>
            <div>
                <ul><li>Google provides API.</li><li>Integrate Nano-Banana into Python scripts.</li><li>Enables automated editing, batch transformations, prompt-driven generation.</li></ul>
                
            </div>
        </details>
        
            </div>
        </details>
        
        <details>
            <summary><strong>Core Functionality: How Nano-Banana Works</strong></summary>
            <div>
                <ul><li>Advanced AI model for high-quality image generation/editing.</li><li>Uses natural language prompts ‚Üí unprecedented control over visual content.</li><li>Colloquially known as Nano-Banana.</li></ul>
                
        <details>
            <summary><strong>Key Features</strong></summary>
            <div>
                <ul><li>Image Generation &amp; Editing: Create/edit images based on textual descriptions.</li><li>Multi-Image Fusion: Combine multiple images ‚Üí single cohesive output.</li><li>Character Consistency: Maintain consistent characters across different images for storytelling.</li><li>Targeted Edits: Make specific alterations (e.g., changing styles, adding elements).</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Vision-Language Model (VLM) Foundation</strong></summary>
            <div>
                <ul><li>Multimodal AI processes visual + textual data simultaneously.</li><li>Fuses text/image processing in unified architecture.</li><li>Stands out for speed, efficiency, understanding/manipulating images w/ natural language.</li></ul>
                
        <details>
            <summary><strong>VLM Components</strong></summary>
            <div>
                <ul><li>Vision Encoder: Processes image ‚Üí numerical representation (embedding); Breaks image ‚Üí smaller patches; Uses Vision Transformer (ViT) to understand patch relationships.</li><li>Language Encoder: Processes text prompt; Tokenizes input words ‚Üí numerical embeddings; Captures meaning/context.</li><li>Fusion Mechanism: Core of VLM; Aligns image embeddings + text embeddings ‚Üí shared &#x27;language&#x27;; Model understands visual/textual info relation (e.g., &#x27;cat&#x27; image ‚Üî &#x27;cat&#x27; text).</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Post-Training VLM Tasks</strong></summary>
            <div>
                <ul><li>Image captioning.</li><li>Visual question answering.</li><li>Image generation/editing (e.g., &#x27;Making 3d-Figures&#x27; from red car image).</li></ul>
                
            </div>
        </details>
        
            </div>
        </details>
        
        <details>
            <summary><strong>Nano Banana&#x27;s Technical Details üçå</strong></summary>
            <div>
                <ul><li>Underlying architecture: advanced VLM built on Google‚Äôs Gemini.</li><li>Designed for efficiency/speed.</li><li>Sub-2-second inference times for image generation.</li></ul>
                
        <details>
            <summary><strong>Unified Multimodal Architecture</strong></summary>
            <div>
                <ul><li>Processes text prompts + visual data together in single transformer system.</li><li>Native fusion ‚Üí better semantic understanding, precise edits.</li><li>Maintains context/realism of original image.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Diffusion-based Generation</strong></summary>
            <div>
                <ul><li>Uses diffusion techniques for image generation.</li><li>Starts w/ noisy image ‚Üí progressively &#x27;denoises&#x27; based on prompt ‚Üí coherent final image.</li><li>Integrates w/ Gemini architecture for enhanced capabilities.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>On-Device &amp; Cloud Optimization</strong></summary>
            <div>
                <ul><li>Requires significant cloud resources for full capabilities.</li><li>Core technology optimized for on-device processing (Gemini Nano model).</li><li>Enables offline features (e.g., image description/summarization on Pixel phone).</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Model Distillation</strong></summary>
            <div>
                <ul><li>Achieves speed/efficiency.</li><li>Smaller, more efficient model learns from larger, complex one.</li><li>Performs complex tasks w/ fewer computational resources.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Context Window</strong></summary>
            <div>
                <ul><li>Massive 1-million token context window.</li><li>Useful for long-range reasoning.</li><li>Editing multiple images while maintaining consistent style/subject identity.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Multimodal Reasoning</strong></summary>
            <div>
                <ul><li>Takes multiple input images ‚Üí synthesizes into single cohesive output.</li><li>Respects physical properties (light/shadows).</li><li>Enables complex tasks (e.g., blending product photo into interior shot).</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>SynthID Watermarking</strong></summary>
            <div>
                <ul><li>All images created/edited embedded w/ invisible SynthID digital watermark.</li><li>Helps identify as AI-generated ‚Üí promotes trust/safety.</li></ul>
                
            </div>
        </details>
        
            </div>
        </details>
        
            </div>
        </details>
        
        <details>
            <summary><strong>Input Types &amp; Output Consistency</strong></summary>
            <div>
                
                
        <details>
            <summary><strong>Supported Input Combinations</strong></summary>
            <div>
                <ul><li>Text Prompt Only: Generate images from descriptive text.</li><li>Image + Text for Editing: Modify existing images based on textual instructions.</li><li>Multiple Images for Fusion: Combine up to five images ‚Üí new composition.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Output Format &amp; Consistency</strong></summary>
            <div>
                <ul><li>Character Identity Maintenance: Consistent appearance of characters across different images.</li><li>Style Retention: Preservation of artistic styles/themes.</li><li>Realism vs. Stylisation Balance: Generates photorealistic + stylised images based on prompts.</li></ul>
                
            </div>
        </details>
        
            </div>
        </details>
        
        <details>
            <summary><strong>Pricing &amp; Token Usage</strong></summary>
            <div>
                <ul><li>Standard Tier: $30 per 1 million output tokens.</li><li>Image Cost: Each image consumes ‚âà 1,290 tokens; Equates to ‚âà $0.039 per image.</li><li>Input Pricing: Text/image inputs priced @ $0.30 per million tokens.</li><li>Example Scenario: Generating 100 images would cost ‚âà $3.90.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Prompt Processing &amp; Best Practices</strong></summary>
            <div>
                
                
        <details>
            <summary><strong>Effective Prompt Elements</strong></summary>
            <div>
                <ul><li>Detail Level: Specific descriptions ‚Üí more accurate results.</li><li>Style Indicators: Mentioning artistic styles guides model‚Äôs output.</li><li>Aspect Ratios: Specifying dimensions ensures image fits desired formats (e.g., 16:9).</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Tips for Crafting Prompts</strong></summary>
            <div>
                <ul><li>Be Hyper-Specific: More detail (subjects, colors, lighting, composition) ‚Üí more control over output.</li><li>Provide Context &amp; Intent: Explain purpose/desired mood; Model‚Äôs understanding influences creative choices.</li><li>Iterate &amp; Refine: Don‚Äôt expect perfection on first try; Use conversational ability for incremental changes.</li><li>Use Step-by-Step Instructions: For complex scenes, break prompt into clear, sequential instructions.</li><li>Use Positive Framing: Describe desired scene positively (e.g., &#x27;empty, deserted street&#x27; instead of &#x27;no cars&#x27;).</li><li>Control the Camera: Use photographic/cinematic terms (e.g., &#x27;wide-angle shot&#x27;, &#x27;macro shot&#x27;, &#x27;low-angle perspective&#x27;).</li></ul>
                
            </div>
        </details>
        
            </div>
        </details>
        
        <details>
            <summary><strong>Python API Integration</strong></summary>
            <div>
                <ul><li>Developers use Google AI Studio for prototyping/experimentation.</li><li>AI Studio is free for Nano Banana.</li><li>Gateway to building applications w/ Gemini API.</li></ul>
                
        <details>
            <summary><strong>Setup Requirements</strong></summary>
            <div>
                <ul><li>API key from Google AI Studio.</li><li>Billing set up for Google Cloud project (API usage is paid).</li><li>Google Gen AI SDK for Python or JavaScript/TypeScript.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Installation</strong></summary>
            <div>
                <ul><li>`pip install -U google-genai`</li><li>`pip install Pillow` (for image manipulation)</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>API Key Generation Steps</strong></summary>
            <div>
                <ul><li>Visit aistudio.google.com ‚Üí sign in.</li><li>Click &#x27;Get API key&#x27; in left navigation.</li><li>Click &#x27;Create API key&#x27;.</li><li>Select existing/create new Google Cloud project (for billing).</li><li>Copy/store displayed API key securely.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>API Usage Examples (Model ID: `gemini-2.5-flash-image-preview`)</strong></summary>
            <div>
                <ul><li>Image Generation from Text: Generate 1+ images from descriptive text prompt.</li><li>Image Editing w/ Text &amp; Image Inputs: Provide existing image + text prompt for edits.</li><li>Photo Restoration: Restore/colorize old photographs w/ simple prompt.</li><li>Working w/ Multiple Input Images: Provide multiple images for complex editing.</li><li>Conversational Image Editing: Use chat session to maintain context across requests for iterative refinement.</li></ul>
                
        <details>
            <summary><strong>Image Generation Example</strong></summary>
            <div>
                <ul><li>Example: `prompt = &#x27;Create a photorealistic image of an orange cat with green eyes, sitting on a couch.&#x27;`</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Image Editing Example</strong></summary>
            <div>
                <ul><li>Excels @ maintaining character/content consistency from input image.</li><li>Example: `prompt = &#x27;Using the image of the cat, create a photorealistic, street-level view of the cat walking along a sidewalk in a New York City neighborhood...&#x27;`</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Photo Restoration Example</strong></summary>
            <div>
                <ul><li>Example: `prompt = &#x27;Restore and colorize this image from 1932&#x27;`</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Multiple Input Images Example</strong></summary>
            <div>
                <ul><li>Example: `prompt = &#x27;Make the girl wear this t-shirt. Leave the background unchanged.&#x27;` (girl.png, tshirt.png)</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Conversational Editing Tip</strong></summary>
            <div>
                <ul><li>If image features degrade/drift after many edits, start new session w/ latest image + detailed, consolidated prompt.</li></ul>
                
            </div>
        </details>
        
            </div>
        </details>
        
            </div>
        </details>
        
        <details>
            <summary><strong>Community &amp; Further Resources</strong></summary>
            <div>
                
                
        <details>
            <summary><strong>Trending Example Prompts (Categories)</strong></summary>
            <div>
                <ul><li>Shifting camera perspective.</li><li>Few-shot learning for consistent character design.</li><li>Google Maps transforms (&#x27;What does the red arrow see&#x27;).</li><li>Generating images from stick figure annotations.</li><li>Creating 3D models from still images.</li><li>Generating location-based AR experiences.</li><li>Converting 2D map into 3D graphic.</li></ul>
                
            </div>
        </details>
        
        <details>
            <summary><strong>Official Resources</strong></summary>
            <div>
                <ul><li>Google AI Studio.</li><li>Gemini API docs.</li><li>Nano Banana Gemini API docs.</li><li>How to prompt Gemini 2.5 Flash Image Generation for best results.</li><li>Nano Banana docs prompting guide.</li><li>Pixshop app in AI Studio.</li></ul>
                
            </div>
        </details>
        
            </div>
        </details>
        
            </div>
        </details>
        
    </div>
    
</div>


<script>
function setMindmapLevel(level) {
    const allDetails = document.querySelectorAll('.mindmap details');

    // Close all
    allDetails.forEach(detail => detail.removeAttribute('open'));

    // Open based on depth
    allDetails.forEach(detail => {
        let depth = 0;
        let parent = detail.parentElement;
        while (parent && parent.tagName !== 'BODY') {
            if (parent.tagName === 'DETAILS') depth++;
            parent = parent.parentElement;
        }
        if (depth < level) detail.setAttribute('open', '');
    });
}

// Automatically open to level 1 on page load
window.addEventListener('DOMContentLoaded', () => {
    setMindmapLevel(1);
});

function dragElement(elmnt) {
    let pos1 = 0, pos2 = 0, pos3 = 0, pos4 = 0;

    const onMouseDown = (e) => {
        e.preventDefault();
        pos3 = e.clientX;
        pos4 = e.clientY;
        document.onmouseup = closeDragElement;
        document.onmousemove = elementDrag;
    };

    const elementDrag = (e) => {
        e.preventDefault();
        pos1 = pos3 - e.clientX;
        pos2 = pos4 - e.clientY;
        pos3 = e.clientX;
        pos4 = e.clientY;
        elmnt.style.top = (elmnt.offsetTop - pos2) + "px";
        elmnt.style.left = (elmnt.offsetLeft - pos1) + "px";
        elmnt.style.transform = "none"; // cancel centering transform
        elmnt.style.bottom = "auto"; // cancel fixed bottom
    };

    const closeDragElement = () => {
        document.onmouseup = null;
        document.onmousemove = null;
    };

    onMouseDown(event);
}
</script>


</body>
</html>
